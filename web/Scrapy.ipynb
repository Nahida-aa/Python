{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Scrapy 中，处理 Spider 模块分析好的结构化数据（如保存入库等）的模块是 **Item Pipeline**。Item Pipeline 允许你对抓取到的数据进行进一步处理，例如清理数据、验证数据、将数据保存到数据库或文件中等。\n",
    "\n",
    "### Item Pipeline 的工作流程\n",
    "\n",
    "1. **定义 Item**：\n",
    "   - 首先，你需要定义一个 Item 类，用于表示抓取到的数据结构。\n",
    "\n",
    "2. **编写 Item Pipeline**：\n",
    "   - 然后，你需要编写一个或多个 Item Pipeline 类，每个类都实现 `process_item` 方法，用于处理 Item。\n",
    "\n",
    "3. **激活 Item Pipeline**：\n",
    "   - 最后，你需要在 Scrapy 项目的配置文件 `settings.py` 中激活 Item Pipeline。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "以下是一个完整的示例，展示了如何使用 Item Pipeline 将抓取到的数据保存到 SQLite 数据库中。\n",
    "\n",
    "#### 定义 Item\n",
    "\n",
    "在 `items.py` 文件中定义一个 Item 类：\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class MyItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    description = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "```\n",
    "\n",
    "#### 编写 Spider\n",
    "\n",
    "在 `spiders` 目录中编写一个 Spider：\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from myproject.items import MyItem\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        item = MyItem()\n",
    "        item['title'] = response.xpath('//title/text()').get()\n",
    "        item['description'] = response.xpath('//meta[@name=\"description\"]/@content').get()\n",
    "        item['url'] = response.url\n",
    "        yield item\n",
    "```\n",
    "\n",
    "#### 编写 Item Pipeline\n",
    "\n",
    "在 `pipelines.py` 文件中编写一个 Item Pipeline 类：\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "\n",
    "class SQLitePipeline:\n",
    "    def open_spider(self, spider):\n",
    "        self.connection = sqlite3.connect('mydatabase.db')\n",
    "        self.cursor = self.connection.cursor()\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS mytable (\n",
    "                title TEXT,\n",
    "                description TEXT,\n",
    "                url TEXT\n",
    "            )\n",
    "        ''')\n",
    "        self.connection.commit()\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.connection.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.cursor.execute('''\n",
    "            INSERT INTO mytable (title, description, url) VALUES (?, ?, ?)\n",
    "        ''', (item['title'], item['description'], item['url']))\n",
    "        self.connection.commit()\n",
    "        return item\n",
    "```\n",
    "\n",
    "#### 激活 Item Pipeline\n",
    "\n",
    "在 `settings.py` 文件中激活 Item Pipeline：\n",
    "\n",
    "```python\n",
    "ITEM_PIPELINES = {\n",
    "    'myproject.pipelines.SQLitePipeline': 300,\n",
    "}\n",
    "```\n",
    "\n",
    "### 解释\n",
    "\n",
    "1. **定义 Item**：\n",
    "   - 在 `items.py` 文件中定义了一个 `MyItem` 类，用于表示抓取到的数据结构。\n",
    "\n",
    "2. **编写 Spider**：\n",
    "   - 在 `spiders` 目录中编写了一个 `MySpider` 类，用于抓取网页数据并生成 `MyItem` 实例。\n",
    "\n",
    "3. **编写 Item Pipeline**：\n",
    "   - 在 `pipelines.py` 文件中编写了一个 `SQLitePipeline` 类，用于将抓取到的数据保存到 SQLite 数据库中。\n",
    "   - `open_spider` 方法在 Spider 开始时调用，创建数据库连接和表。\n",
    "   - `close_spider` 方法在 Spider 结束时调用，关闭数据库连接。\n",
    "   - `process_item` 方法处理每个抓取到的 Item，将其插入数据库。\n",
    "\n",
    "4. **激活 Item Pipeline**：\n",
    "   - 在 `settings.py` 文件中激活 `SQLitePipeline`，并设置其优先级为 300。\n",
    "\n",
    "### 总结\n",
    "\n",
    "- **Item Pipeline** 是 Scrapy 中用于处理 Spider 模块分析好的结构化数据的模块。\n",
    "- 通过定义 Item、编写 Item Pipeline 类和激活 Item Pipeline，可以对抓取到的数据进行进一步处理，如保存到数据库或文件中。\n",
    "\n",
    "通过理解和使用 Item Pipeline，你可以更有效地处理和保存抓取到的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
